import Callout from 'nextra-theme-docs/callout'
import Image from 'next/image'

# What's the point of data structures?

We want a computer to do some useful computation. A computation is an algorithm, which is 
"a finite sequence of rigorous instructions" [Wikipedia](https://en.wikipedia.org/wiki/Algorithm). 

Typically, we want to run a set of instructions to tell us something about information (data) we already have.
We might also want to transform that data to yield new or modified data.

That's where data structures come into play. Data structures say how can I organize my data to 
allow me to write the most efficient (i.e., fast completing) set of instructions. 

# How do we talk about data structures 

In computer science, there is often a distinction made between interface (operations / abstract data type) 
and implementation (algorithms and internal representation of data needed for operations).

# Common interfaces

| Type      | Note      |
|-----------|-----------|
| List (includes Stack, Deque, Queue) | Sequence indexed by 0,1,2,...,n-1, implementations change behavior for adding or removing from the list |
| Unordered Sets (includes Dictionaries, Maps) | Distinct elements in no particular order |
| Sorted Sets | Provides ability to compare elements to determine ordering, find(x) more meaningful as you say find smallest y such that y >= x. Return y or null if does not exist. Slower runtime on find though versus Unsorted|

# Mathematical background

These notes are brief. If there is time, in the future, could be good to look through [this](https://people.csail.mit.edu/meyer/mcs.pdf). Have not 
yet had time to do so, but found based on citations for this book and a quick web search.

## Exponents, logarithms, and factorials

For some positive integer $b$, we can say that $b^x = b \times b \times \cdots \times b$ 
where the right hand side has $b$ $x$ times. 

$b^0 = 1$

$b^x = \frac{1}{b^{-x}}$

<Callout emoji="ðŸ’­">
 Might think of log as scaling down whereas exponent scales up (probably should research that claim more...).
</Callout>


$x = log_bk$ means $x$ is the "base-$b$ logarithm of $k$". 

In other $b^x = k$. 

Informally, $x$ says how many times can we divide $k$ by $b$ until
the result is less than or equal to 1.

The natural log $log_e =$ ln$k$. 

$e = \lim_{\ n \to \infty} \ (1+\frac{1}{n})^n$ 

Care about ln$k$ because solves integral for $\frac{1}{x}dx$ 
from 1 to $k$. 


Exponents and logarithms deal with scaling, and we can use factorials
for counting.

For a non-negative integer $n$, $n! = 1 \cdot 2 \cdot 3 \cdots n$. 

This gives the number of distinct orderings (i.e., permutations) of n elements.

If instead we wanted to say how many subsets can we make from $n$ of size $k$, we can
do $n$ choose $k$ to get a binomial coefficient. 

${n \choose k} = \frac{n!}{k!(n-k)}$. 

<Callout emoji="ðŸ’­">
$n$ choose $k$ (i.e., number of distinct subsets) is smaller than permutations of $n$ (i.e., distinct orderings)
</Callout>


## Asymptotic notation (run time)

Time to execute = number of instructions run NOT total time in seconds
for instructions to execute.

To further simplify, use big-Oh notation.

$O(f(n)) = $ $g(n)$ such that there exists $c>0$, and $n_{0}$ such that
$g(n) \leq c \cdot f(n)$ for all $n \geq n_{0}$. 

Note! $O(f(n))$ is actually a set, not some number or a function like $f(x) = x^2$.

For ease, may often say $f_{1}(n) = O(f(n))$ but properly, this would be $ f_1(n) \in O(f(n))$.

The above also implies simplification. In other words, different functions can all be members of the 
same big-Oh set. This comes from the definition of big-Oh notation!

It only needs to be true that for whatever $g(n)$ we pick, that $f(n)$ dominates $g(n)$. 

For example $f(n) = 5n\log n + 8n - 200$. 
Then it is true that $f(n) \leq c\cdot n\log n \implies f(n) \in O(n\log n)$,
where we can pick $c$ to be a big enough integer constant for the above to hold true.

The definition of big-Oh also implies that one $f(n)$ can be in multiple 
big-Oh sets. 

$f(n)$ from above $\leq n\log n$ but also $\leq c^n$ for some sufficiently large $c$. 

This implies $f(n) \in O(n log n)$ and $f(n) \in O(c^n)$. 

Which leads to another implication! Some sets in big-Oh can be subsets of each other.

For any $c_1 < c_2$ and constants $a,b,c > 0$: 

 - $O(n^{c_1}) < O(n^{c_2})$ 
 - $O(a) \subset O(\log n) \subset O(n^b) \subset O(c^n)$
 - multiplying above by positive values still hold so: $O(n) \subset O(n \log n)$ for example.

<Callout emoji="ðŸ’­">
Big-Oh 'sets' is my own language. I'm not positive if that's the right way
to say things. Big-Oh notation lets us simplify problems into groups with similar efficiency levels 
in terms of the number of instructions that need to be executed. 
</Callout>

If two algorithms share the same big-Oh notation (i.e., are in the same big-Oh set), it 
can't be said which algorithm is faster.

<Callout emoji="ðŸ’­">
Below is my attempt at an example. It should not be considered
a formal proof, but rather as an attempt to demonstrate the general
utility of big-Oh. Also for now, I have excluded discussion of multivariate functions in relation to
asymptotic (big-Oh) notation from here.
</Callout>

If two algorithms have different big-Oh notation, then it can be reasoned
if one is faster than the other assuming some $n$ big enough.

For example if $f_1(n) \in O(n)$ and $f_2(n) \in O(n\log n)$ then we know that
$n \leq c_1 \cdot f_1(n)$ and $n \log n \leq c_2 \cdot f_2(n)$ for $c_1, c_2 > 0$.

Also, taking as given that $n \leq n log n$ for $n > 0$, then for some $n$ big enough,
$n \leq c_1 \cdot f_1(n) \leq n \log n \leq c_2 f_2(n)$.

Simplifying the above we have $c_1 \cdot f_1(n) \leq c_2 f_2(n)$. 

## Randomization and probability

Some data structures may make random choices, which means operations
don't always run in the same amount of time.

Then, look at the expected (i.e., predicted average) running time for an operation. 

For a discrete random variable $X$ in universe $U$, 
define expected value as $E[X] = \displaystyle\sum_{x \in U} x \cdot Pr[X=x]$.

In words, the expected value of a random variable is the sum of each possible value $X$ can take
times the chance that value occurs.

Also, expectations are linear! More complicated problems can be broken down
into simpler ones.

$E[X + Y] = E[X] + E[Y]$. Generalized, find that for random variables
$X_1, \ldots, X_k$ have $E[\displaystyle\sum_{i=1}^k X_i] = \displaystyle\sum_{i=1}^k E[X_i]$.

Linearity makes it easier to work with indicator random variables. 
Indicator random variables are those that predict the count of something.

For example, a heads/tails coin flipped $k$ times has an expected number of heads
$k/2$. 

For $i \in \{1,\ldots,k\}$, let an indicator variable 
$I_i$ be $1$ if the $ith$ coin toss is heads or $0$ otherwise.

Then, $E[I_i] = (1/2)1 + (1/2)0 = 1/2$. 

Using linearity, $E[X] = E[\displaystyle\sum_{i=1}^k I_i] = \displaystyle\sum_{i=1}^k E[I_i] = \displaystyle\sum_{i=1}^k (1/2) = k/2$

<Callout emoji="ðŸ’­">
One thing that I find unintuitive but which is very powerful is that linearity of expectation holds
even if random variables $X$ and $Y$ are somehow related to each other 
(i.e., what happens in $X$ impacts what happens in $Y$ or vice versa)! 
Further detail to this is outside the scope of these notes, but a quick web search
will confirm that fact for you. 
Here's two supplemental confirmation sources: [Source 1](https://courses.cs.washington.edu/courses/cse312/19sp/schedule/lecture13.pdf) and [Source 2](https://dlsun.github.io/probability/linearity.html).
</Callout>

## The model of computation 

These notes are based on a model of computation. The model selected is a 
w-bit word-RAM model. 

RAM = Random Access Memory. 

Implication of the above: basic operations on words take constant time
(i.e., arithmetic operations, comparisons, and bitwise boolean operations).

Reading a cell (i.e.,  a w-bit word) happens in constant time.

Assume that $w \geq log n$. Where $n$ is the number of elements in any data structure. 

Assume that all data structures studied are of generic type $T$, where each element
of type $T$ occupies one word of memory. 

Allocating memory takes $O(k)$ time where $k$ is the size of the memory allocated,
and what is returned is a reference to that memory block.

This reference is called a pointer! Pointers are small enough to 
be represented by a single word. 

<Callout emoji="ðŸ’­">
Using pointers allows you to find and work with arbitrarily sized
memory blocks.
</Callout>


## Correctness, time complexity, and space complexity

- Correctness: data structure correctly implements interface
- Time complexity: running time (think big-Oh notation) of operations on data structure
 - running time types:
    - worst-case: strongest running time guarantee. Operations never take longer than $f(n)$
    - amortized: $m$ operations may take at most $mf(n)$ time. Implies that on average, take $f(n)$ time
    - expected: running time is a random variable, $f(n)$ is the expected value of the random variable
- Space complexity: data structure should use as little memory as possible

Consider that worst-case running times might often be worse than amortized running times.
If can accept the risk that $actual$ running time is worse than $expected$, may make sense to use amortized 
running time in analysis. 

Assume for these notes, correctness is true. Data structures don't perform actions
improperly.

## Open Data Structures dependency graph

The below shows how sections in the Open Data Structures
textbook relate. 

Dashed arrow = weak dependency. 

<Image src="/ODS/Introduction/ODS-Introduction-1.png" alt="ODS dependency graph" width={500} height={750} />