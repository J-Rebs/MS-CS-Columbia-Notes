import Callout from 'nextra-theme-docs/callout'

# What's the point of data structures?

We want a computer to do some useful computation. A computation is an algorithm, which is 
"a finite sequence of rigorous instructions" [Wikipedia](https://en.wikipedia.org/wiki/Algorithm). 

Typically, we want to run a set of instructions to tell us something about information (data) we already have.
We might also want to transform that data to yield new or modified data.

That's where data structures come into play. Data structures say how can I organize my data to 
allow me to write the most efficient (i.e., fast completing) set of instructions. 

# How do we talk about data structures 

In computer science, there is often a distinction made between interface (operations / abstract data type) 
and implementation (algorithms and internal representation of data needed for operations).

# Common interfaces

| Type      | Note      |
|-----------|-----------|
| List (includes Stack, Deque, Queue) | Sequence indexed by 0,1,2,...,n-1, implementations change behavior for adding or removing from the list |
| Unordered Sets (includes Dictionaries, Maps) | Distinct elements in no particular order |
| Sorted Sets | Provides ability to compare elements to determine ordering, find(x) more meaningful as you say find smallest y such that y >= x. Return y or null if does not exist. Slower runtime on find though versus Unsorted|

# Mathematical background (ðŸ¤“)

These notes are brief. If there is time, in the future, could be good to look through [this](https://people.csail.mit.edu/meyer/mcs.pdf). Have not 
yet had time to do so, but found based on citations for this book and a quick web search.

## Exponents, Logarithms, and Factorials

For some positive integer $b$, we can say that $b^x = b \times b \times \cdots \times b$ 
where the right hand side has $b$ $x$ times. 

$b^0 = 1$

$b^x = \frac{1}{b^{-x}}$

<Callout emoji="ðŸ’­">
 Might think of log as scaling down whereas exponent scales up (probably should research that claim more...).
</Callout>


$x = log_bk$ means $x$ is the "base-$b$ logarithm of $k$". 

In other $b^x = k$. 

Informally, $x$ says how many times can we divide $k$ by $b$ until
the result is less than or equal to 1.

The natural log $log_e =$ ln$k$. 

$e = \lim_{\ n \to \infty} \ (1+\frac{1}{n})^n$ 

Care about ln$k$ because solves integral for $\frac{1}{x}dx$ 
from 1 to $k$. 


Exponents and logarithms deal with scaling, and we can use factorials
for counting.

For a non-negative integer $n$, $n! = 1 \cdot 2 \cdot 3 \cdots n$. 

This gives the number of distinct orderings (i.e., permutations) of n elements.

If instead we wanted to say how many subsets can we make from $n$ of size $k$, we can
do $n$ choose $k$ to get a binomial coefficient. 

${n \choose k} = \frac{n!}{k!(n-k)}$. 

<Callout emoji="ðŸ’­">
$n$ choose $k$ (i.e., number of distinct subsets) is smaller than permutations of $n$ (i.e., distinct orderings)
</Callout>


## Asymptotic Notation (run time)

Time to execute = number of instructions run NOT total time in seconds
for instructions to execute.

To further simplify, use big-Oh notation.

$O(f(n)) = $ $g(n)$ such that there exists $c>0$, and $n_{0}$ such that
$g(n) \leq c \cdot f(n)$ for all $n \geq n_{0}$. 

Note! $O(f(n))$ is actually a set, not some number or a function like $f(x) = x^2$.

For ease, may often say $f_{1}(n) = O(f(n))$ but properly, this would be $ f_1(n) \in O(f(n))$.

The above also implies simplification. In other words, different functions can all be members of the 
same big-Oh set. This comes from the definition of big-Oh notation!

It only needs to be true that for whatever $g(n)$ we pick, that $f(n)$ dominates $g(n)$. 

For example $f(n) = 5n\log_n + 8n - 200$. 
Then it is true that $f(n) \leq c\cdot n\log_n \implies f(n) \in O(n\log_n)$,
where we can pick $c$ to be a big enough integer constant for the above to hold true.

The definition of big-Oh also implies that one $f(n)$ can be in multiple 
big-Oh sets. 

$f(n)$ from above $\leq n\log_n$ but also $\leq c^n$ for some sufficiently large $c$. 

This implies $f(n) \in O(n log_n)$ and $f(n) \in O(c^n)$. 

Which leads to another implication! Some sets in big-Oh can be subsets of each other.

For any $c_1 < c_2$ and constants $a,b,c > 0$: 

 - $O(n^{c_1}) < O(n^{c_2})$ 
 - $O(a) \subset O(\log_n) \subset O(n^b) \subset O(c^n)$
 - multiplying above by positive values still hold so: $O(n) \subset O(n \log_n)$ for example.

<Callout emoji="ðŸ’­">
Big-Oh 'sets' is my own language. I'm not positive if that's the right way
to say things. Big-Oh notation lets us simplify problems into groups with similar efficiency levels 
in terms of the number of instructions that need to be executed. 
</Callout>

If two algorithms share the same big-Oh notation (i.e., are in the same big-Oh set), it 
can't be said which algorithm is faster.

<Callout emoji="ðŸ’­">
Below is my attempt at an example. It should not be considered
a formal proof, but rather as an attempt to demonstrate the general
utility of big-Oh. Also for now, I have excluded discussion of multivariate functions in relation to
asymptotic (big-Oh) notation from here.
</Callout>

If two algorithms have different big-Oh notation, then it can be reasoned
if one is faster than the other assuming some $n$ big enough.

For example if $f_1(n) \in O(n)$ and $f_2(n) \in O(n\log_n)$ then we know that
$n \leq c_1 \cdot f_1(n)$ and $n \log_n \leq c_2 \cdot f_2(n)$ for $c_1, c_2 > 0$.

Also, taking as given that $n \leq n log_n$ for $n > 0$, then for some $n$ big enough,
$n \leq c_1 \cdot f_1(n) \leq n \log_n \leq c_2 f_2(n)$.

Simplifying the above we have $c_1 \cdot f_1(n) \leq c_2 f_2(n)$. 

